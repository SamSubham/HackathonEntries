{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x00000262C9A350B8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n",
      "C:\\Users\\Sam\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "C:\\Users\\Sam\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "C:\\Users\\Sam\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "C:\\Users\\Sam\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start CV optimization\n",
      "{'colsample_bytree': 0.2, 'count_n': 0, 'cv_n': 4, 'eta': 0.03, 'gamma': 0, 'max_depth': 13, 'mc_test': True, 'n_monte_carlo': 1, 'nthread': 3, 'num_class': 3, 'num_round': 10000, 'objective': 'multi:softmax', 'silent': 1, 'subsample': 0.8, 'test_rounds_fac': 1}\n",
      "There are 39 columns\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'communal standpipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e0d8f42abb8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# train machine learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mxg_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[0mxg_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[1;34m(self, mat, missing, nthread)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;31m# we try to avoid data copies if possible (reshape returns a view when possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;31m# and we explicitly tell np.array to try and avoid copying)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'communal standpipe'"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datetime\n",
    "\n",
    "\n",
    "def date_parser(df):\n",
    "    date_recorder = list(map(lambda x: datetime.datetime.strptime(str(x), '%Y-%m-%d'),\n",
    "                             df['date_recorded'].values))\n",
    "    df['year_recorder'] = list(map(lambda x: int(x.strftime('%Y')), date_recorder))\n",
    "    df['weekday_recorder'] = list(map(lambda x: int(x.strftime('%w')), date_recorder))\n",
    "    df['yearly_week_recorder'] = list(map(lambda x: int(x.strftime('%W')), date_recorder))\n",
    "    df['month_recorder'] = list(map(lambda x: int(x.strftime('%m')), date_recorder))\n",
    "    df['age'] = df['year_recorder'].values - df['construction_year'].values\n",
    "    del df['date_recorded']\n",
    "    return df\n",
    "\n",
    "\n",
    "def col_to_freq(df, col_names):\n",
    "    for col in col_names:\n",
    "        print('Changing to frequency %s' %col)\n",
    "        val_counts = df[col].value_counts()\n",
    "        df[col + '_freq'] = np.zeros((df.shape[0],))\n",
    "        for i, val in enumerate(df[col].values):\n",
    "            df[col + '_freq'].iat[i] = int(val_counts.at[val])\n",
    "    return df\n",
    "\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    \"\"\"\n",
    "    accuracy calculation function for xgboost\n",
    "    :param preds: predictions\n",
    "    :param dtrain: labels\n",
    "    :return: -1 * accuracy (for minimization)\n",
    "    \"\"\"\n",
    "    labels = dtrain.get_label()\n",
    "    # return a pair metric_name, result\n",
    "    # since preds are margin(before logistic transformation, cutoff at 0)\n",
    "    return 'Accuracy', -1 * float(sum(labels == preds)) / len(labels)\n",
    "\n",
    "\"\"\"\n",
    "Import data\n",
    "\"\"\"\n",
    "train = pd.DataFrame.from_csv('C:/Users/Sam/Desktop/AEGIS Classes/ML_whole/DataDriven/train.csv')\n",
    "train_index = train.index.values\n",
    "test = pd.DataFrame.from_csv('C:/Users/Sam/Desktop/AEGIS Classes/ML_whole/DataDriven/test.csv')\n",
    "test_index = test.index.values\n",
    "\n",
    "# combing tran and test data\n",
    "# helps working on all the data and removes factorization problems between train and test\n",
    "dataframe = pd.concat([train, test], axis=0)\n",
    "\n",
    "train_labels = pd.DataFrame.from_csv('C:/Users/Sam/Desktop/AEGIS Classes/ML_whole/DataDriven/trainlabels.csv')\n",
    "\n",
    "submission_file = pd.DataFrame.from_csv(\"C:/Users/Sam/Desktop/AEGIS Classes/ML_whole/DataDriven/SubmissionFormat.csv\")\n",
    "\n",
    "\"\"\"\n",
    "Preprocess: date parsing already done\n",
    "\"\"\"\n",
    "# Change labels to ints in order to use as y vector\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels.iloc[:, 0] = label_encoder.fit_transform(train_labels.values.flatten())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Split into train and test\n",
    "\"\"\"\n",
    "\n",
    "train = dataframe.loc[train_index]\n",
    "test = dataframe.loc[test_index]\n",
    "\n",
    "\"\"\"\n",
    "CV\n",
    "\"\"\"\n",
    "best_score = 0\n",
    "best_params = 0\n",
    "best_train_prediction = 0\n",
    "best_prediction = 0\n",
    "meta_solvers_train = []\n",
    "meta_solvers_test = []\n",
    "best_train = 0\n",
    "best_test = 0\n",
    "\n",
    "# Optimization parameters\n",
    "early_stopping = 150\n",
    "param_grid = [\n",
    "              # For optimization\n",
    "              # {\n",
    "              #  'silent': [1],\n",
    "              #  'nthread': [3],\n",
    "              #  # 'eval_metric': ['evalerror'],\n",
    "              #  'eta': [0.1],\n",
    "              #  'objective': ['multi:softmax'],\n",
    "              #  'max_depth': [6],\n",
    "              #  'num_round': [10000],\n",
    "              #  'gamma': [0],\n",
    "              #  'subsample': [0.8],\n",
    "              #  'colsample_bytree': [0.3],\n",
    "              #  'n_monte_carlo': [1],\n",
    "              #  'cv_n': [4],\n",
    "              #  'test_rounds_fac': [1],\n",
    "              #  'count_n': [0],\n",
    "              #  'mc_test': [True],\n",
    "              #  'num_class': [3]\n",
    "              #  },\n",
    "              # For final calculation\n",
    "              {\n",
    "               'silent': [1],\n",
    "               'nthread': [3],\n",
    "               # 'eval_metric': ['evalerror'],\n",
    "               'eta': [0.03],\n",
    "               'objective': ['multi:softmax'],\n",
    "               'max_depth': [13],\n",
    "               'num_round': [10000],\n",
    "               'gamma': [0, 1, 2, 4, 8, 16],\n",
    "               'subsample': [0.8],\n",
    "               'colsample_bytree': [0.2],\n",
    "               'n_monte_carlo': [1],\n",
    "               'cv_n': [4],\n",
    "               'test_rounds_fac': [1],\n",
    "               'count_n': [0],\n",
    "               'mc_test': [True],\n",
    "               'num_class': [3]\n",
    "               }\n",
    "              ]\n",
    "\n",
    "print('start CV optimization')\n",
    "mc_round_list = []\n",
    "mc_acc_mean = []\n",
    "mc_acc_sd = []\n",
    "params_list = []\n",
    "print_results = []\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(params)\n",
    "    params_list.append(params)\n",
    "    train_predictions = np.ones((train.shape[0],))\n",
    "    print('There are %d columns' % train.shape[1])\n",
    "\n",
    "    # CV\n",
    "    mc_auc = []\n",
    "    mc_round = []\n",
    "    mc_train_pred = []\n",
    "    # Use monte carlo simulation if needed to find small improvements\n",
    "    for i_mc in range(params['n_monte_carlo']):\n",
    "        cv_n = params['cv_n']\n",
    "        kf = StratifiedKFold(train_labels.values.flatten(), n_folds=cv_n, shuffle=True, random_state=i_mc ** 3)\n",
    "\n",
    "        xgboost_rounds = []\n",
    "        # Finding optimized number of rounds\n",
    "        for cv_train_index, cv_test_index in kf:\n",
    "            X_train, X_test = train.values[cv_train_index, :], train.values[cv_test_index, :]\n",
    "            y_train = train_labels.iloc[cv_train_index].values.flatten()\n",
    "            y_test = train_labels.iloc[cv_test_index].values.flatten()\n",
    "\n",
    "            # train machine learning\n",
    "            xg_train = xgboost.DMatrix(X_train, label=y_train)\n",
    "            xg_test = xgboost.DMatrix(X_test, label=y_test)\n",
    "\n",
    "            watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "\n",
    "            num_round = params['num_round']\n",
    "            xgclassifier = xgboost.train(params, xg_train, num_round, watchlist, early_stopping_rounds=early_stopping,\n",
    "                                         feval=evalerror);\n",
    "            xgboost_rounds.append(xgclassifier.best_iteration)\n",
    "\n",
    "        num_round = int(np.mean(xgboost_rounds))\n",
    "        print('The best n_rounds is %d' % num_round)\n",
    "\n",
    "        # Calculate train predictions over optimized number of rounds\n",
    "        for cv_train_index, cv_test_index in kf:\n",
    "            X_train, X_test = train.values[cv_train_index, :], train.values[cv_test_index, :]\n",
    "            y_train = train_labels.iloc[cv_train_index].values.flatten()\n",
    "            y_test = train_labels.iloc[cv_test_index].values.flatten()\n",
    "\n",
    "            # train machine learning\n",
    "            xg_train = xgboost.DMatrix(X_train, label=y_train)\n",
    "            xg_test = xgboost.DMatrix(X_test, label=y_test)\n",
    "\n",
    "            watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "\n",
    "            xgclassifier = xgboost.train(params, xg_train, num_round, watchlist, feval=evalerror);\n",
    "\n",
    "            # predict\n",
    "            predicted_results = xgclassifier.predict(xg_test)\n",
    "            train_predictions[cv_test_index] = predicted_results\n",
    "\n",
    "        print('Accuracy score ', accuracy_score(train_labels.values, train_predictions))\n",
    "        mc_auc.append(accuracy_score(train_labels.values, train_predictions))\n",
    "        mc_train_pred.append(train_predictions)\n",
    "        mc_round.append(num_round)\n",
    "\n",
    "    # Getting the mean integer\n",
    "    mc_train_pred = (np.mean(np.array(mc_train_pred), axis=0) + 0.5).astype(int)\n",
    "\n",
    "    mc_round_list.append(int(np.mean(mc_round)))\n",
    "    mc_acc_mean.append(np.mean(mc_auc))\n",
    "    mc_acc_sd.append(np.std(mc_auc))\n",
    "    print('The accuracy range is: %.5f to %.5f and best n_round: %d' %\n",
    "          (mc_acc_mean[-1] - mc_acc_sd[-1], mc_acc_mean[-1] + mc_acc_sd[-1], mc_round_list[-1]))\n",
    "    print_results.append('The accuracy range is: %.5f to %.5f and best n_round: %d' %\n",
    "                         (mc_acc_mean[-1] - mc_acc_sd[-1], mc_acc_mean[-1] + mc_acc_sd[-1], mc_round_list[-1]))\n",
    "    print('For ', mc_auc)\n",
    "    print('The accuracy of the average prediction is: %.5f' % accuracy_score(train_labels.values, mc_train_pred))\n",
    "    meta_solvers_train.append(mc_train_pred)\n",
    "\n",
    "    # train machine learning\n",
    "    xg_train = xgboost.DMatrix(train.values, label=train_labels.values)\n",
    "    xg_test = xgboost.DMatrix(test.values)\n",
    "\n",
    "    # predicting the test set\n",
    "    if params['mc_test']:\n",
    "        watchlist = [(xg_train, 'train')]\n",
    "\n",
    "        num_round = int(mc_round_list[-1] * params['test_rounds_fac'])\n",
    "        mc_pred = []\n",
    "        for i_mc in range(params['n_monte_carlo']):\n",
    "            params['seed'] = i_mc\n",
    "            xg_train = xgboost.DMatrix(train, label=train_labels.values.flatten())\n",
    "            xg_test = xgboost.DMatrix(test)\n",
    "\n",
    "            watchlist = [(xg_train, 'train')]\n",
    "\n",
    "            xgclassifier = xgboost.train(params, xg_train, num_round, watchlist, feval=evalerror);\n",
    "            predicted_results = xgclassifier.predict(xg_test)\n",
    "            mc_pred.append(predicted_results)\n",
    "\n",
    "        meta_solvers_test.append((np.mean(np.array(mc_pred), axis=0) + 0.5).astype(int))\n",
    "        \"\"\" Write opt solution \"\"\"\n",
    "        print('writing to file')\n",
    "        mc_train_pred = label_encoder.inverse_transform(mc_train_pred.astype(int))\n",
    "        print(meta_solvers_test[-1])\n",
    "        meta_solvers_test[-1] = label_encoder.inverse_transform(meta_solvers_test[-1])\n",
    "        pd.DataFrame(mc_train_pred).to_csv('results/train_xgboost_d13.csv')\n",
    "        submission_file['status_group'] = meta_solvers_test[-1]\n",
    "        submission_file.to_csv(\"results/test_xgboost_d13.csv\")\n",
    "\n",
    "    # saving best score for printing\n",
    "    if mc_acc_mean[-1] < best_score:\n",
    "        print('new best log loss')\n",
    "        best_score = mc_acc_mean[-1]\n",
    "        best_params = params\n",
    "        best_train_prediction = mc_train_pred\n",
    "        if params['mc_test']:\n",
    "            best_prediction = meta_solvers_test[-1]\n",
    "\n",
    "print(best_score)\n",
    "print(best_params)\n",
    "\n",
    "print(params_list)\n",
    "print(print_results)\n",
    "print(mc_acc_mean)\n",
    "print(mc_acc_sd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
